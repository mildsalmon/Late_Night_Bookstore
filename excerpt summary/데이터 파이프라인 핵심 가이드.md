# 01장: 데이터 파이프라인 소개

데이터의 진정한 가치는 그것이 정제되어 소비자에게 전달된 후의 잠재력에 있다. 가치 사슬의 각 단계를 통해 데이터를 전달하려면 효율적인 파이프라인이 필요하다.

이 책에서는 데이터 파이프라인에 대한 설명과 현대의 데이터 생태계에 어떻게 적용되는지 보여준다. 일괄 처리 vs 스트리밍 데이터 수집, 직접 구축 vs 제품을 구매하는 것 (on premise vs cloud를 말하는 것일까?) 등과 같이 파이프라인을 구현할 때의 일반적인 고려 사항과 주요 결정 사항을 다룬다.

### A. 데이터 파이프라인이란?

데이터 파이프라인은 다양한 소스에서 개로운 가치를 얻을 수 있는 대상으로 데이터를 옮기고 변환하는 일련의 과정이다.

가장 단순한 형태의 파이프라인은 REST API처럼 단일 소스에서 데이터를 추출하고 데이터 웨어하우스의 SQL 테이블과 같은 대상으로 데이터를 로드하는 것이다. 그러나 실제로 파이프라인은 일반적으로 데이터 추출, 데이터 가공, 데이터 유효성 검사를 포함한 여러 단계로 구성되며, 때로는 데이터를 최종 목적지로 전달하기 전에 머신러닝 모델을 학습하거나 실행하는 단계가 있기도 하다.

[그림 1-1]

### B. 누가 파이프라인을 구축할까?

데이터 엔지니어는 분석 생태계를 뒷받침하는 데이터 파이프라인을 구축하고 유지관리하는 데 전문적인 역량을 갖추고 있다.

데이터 엔지니어의 목적은 단순히 데이터를 데이터 웨어하우스에 로드하는 것이 아니다. 데이터 엔지니어는 데이터 과학자 및 분석가와 긴밀히 협력하여 데이터를 어떻게 처리해야 하는지 파악하고 요구사항을 확장 가능한 프로덕션 상태로 전환하는 데 도움을 준다.

데이터 엔지니어는 제공하는 데이터의 유효성과 적시성을 보장하는 데 자부심을 가지고 있다. 무엇인가 잘못되었을 떄를 대비하여 테스트, 경고 및 비상 계획을 수립한다.

##### a. SQL과 데이터 웨어하우징 기초

숙련된 데이터 엔지니어는 고성능의 SQL 작성 방법을 알고 데이터 웨어하우징 및 데이터 모델링의 기본 사항을 이해한다.

##### b. 파이썬 그리고/또는 자바

팀에서 사용하는 기술 스택에 따라 달라지지만, 결국 코드 없이 작업을 수행할 수는 없다. 파이썬과 자바가 데이터 엔지니어링에서 우위를 점하고 있지만 고(Go)와 같은 언어도 등장하고 있다.

##### c. 분산 컴퓨팅

분산 컴퓨팅은 여러 시스템의 성능을 결합하여 대량의 데이터를 효율적으로 저장, 처리 및 분석한다.

분석에서 분산 컴퓨팅의 대표적인 예는 하둡 분산 파일 시스템(HDFS)을 통한 분산 파일 스토리지, 맵리듀스를 통한 처리, 피그(pig)를 통한 데이터 분석 등을 포함하는 하둡 에코시스템이다. 아파치 스파크는 하둡을 빠르게 능가하는 또 다른 인기 분산 처리 프레임워크다.

데이터 엔지니어는 분산 컴퓨팅을 언제 어떻게 활용해야 하는지 알아야 한다.

##### d. 기본 시스템 관리

데이터 엔지니어는 리눅스 명령줄에 능숙해야 하며 로그 분석, 크론 작업 예약, 방화벽 및 기타 보안 설정 문제 해결을 수행할 수 있어야 한다. AWS, Azure, Google Cloud와 같은 클라우드 제공업체에서 전반적으로 작업하는 경우 위 기술을 사용해서 클라우드 서비스와 함께 데이터 파이프라인을 배포하게 된다.

##### e. 목표 지향적 사고방식

팀의 분석가와 데이터 과학자와 정기적으로 만나서 파이프라인을 구축하는 이유를 알 때 더 나은 아키텍처 결정을 내릴 수 있다.

### C. 왜 데이터 파이프라인을 구축할까?

분석 워크플로의 최종 결과물만이 대다수 조직이 보는 전부다.

데이터 분석가가 생성하는 모든 대시보드와 통찰력, 그리고 데이터 과학자가 개발한 각 예측 모델에는 뒷단에서 작동하는 데이터 파이프라인이 있다. 단일 대시보드 또는 단일 지표가 여러 소스 시스템에서 생성되는 데이터에서 파생되는 경우는 아주 흔하다. 또한 데이터 파이프라인은 단순히 원본에서 데이터를 추출하여 분석가가 사용할 수 있도록 단순한 데이터베이스 테이블이나 플랫 파일로 로드하는 것 이상을 수행한다. 원본 데이터는 정리, 정형화, 정규화, 결합, 집계 때로는 마스킹 또는 보안을 위해 정제된다.

### D. 어떻게 데이터 파이프라인을 구축할까?

이 책을 통해 파이프라인 구축을 위한 가장 인기 있는 솔루션 및 프레임워크를 살펴보고 조직의 요구 사항과 제약 조건에 따라 어떤 제품을 사용할지 결정하는 방법을 알아본다.

파이프라인은 단순히 구축되는 것이 아니라 모니터링, 유지 관리 및 확장된다. 데이터 엔지니어는 데이터를 한 번만 제공하는 것이 아니라 파이프라인을 구축하고 이를 안정적이고 안전하게 제시간에 제공하고 처리하는 인프라를 지원해야 한다. 작으 일은 아니지만 잘 수행하면 조직 데이터의 가치를 진정으로 실현할 수 있다.
    
# 02장: 최신 데이터 인프라

파이프라인을 구축하기 위한 제품과 설계를 결정하기 전에 최신 데이터 스택을 구성하는 요소를 이해할 필요가 있다. ... 업계 표준이 되어 파이프라인 구현에 있어 모범 사례의 발판을 마련한 몇 가지 핵심 요구 사항과 개념은 있다.

### 데이터 소스의 다양성

[그림 2-1]

##### 소스 시스템 소유권

> 데이터 수집(data ingestion)이라는 용어는 한 소스에서 데이터를 추출하여 다른 소스로 로드하는 것을 의미한다.

내부적으로 구축된 시스템은 액세스 방법뿐만 아니라 데이터를 사용자가 필요로 하는 형태에 맞추어 정의하는 등의 더 많은 기회를 분석 팀에 제공할 수 있다. 시스템을 구축할 때 데이터 수집을 고려하여 설계했는지는 또 다른 문제다. 데이터 수집이 시스템에 의도하지 않은 부하를 가하는지부터 데이터를 점진적(incremental)으로 로드할 수 있는지 여부까지 다양한 과제가 발생하기 때문이다. (OLTP DB에서 OLAP 쿼리를 날리는 것?)

##### 수집 인터페이스 및 데이터 구조

데이터 엔지니어가 새로운 데이터 수집을 구축할 때 데이터 엔지니어가 가장 먼저 알아볼 것은 소스 데이터를 얻는 방법과 형식이다.

- 데이터에 대한 인터페이스를 살펴보기
	- Postgres 또는 MySQL 데이터베이스와 같은 애플리케이션 뒤에 있는 데이터베이스
	- REST API와 같은 시스템 상단의 추상화 계층
	- Apache Kafka와 같은 스트림 처리 플랫폼
	- 로그, 쉼표로 구분된 값(csv) 파일 및 기타 플랫 파일을 포함하는 공유 네트워크 파일 시스템(NFS) 또는 클라우드 스토리지 버킷
	- 데이터 웨어하우스 또는 데이터 레이크
	- HDFS 또는 HBase 데이터베이스의 데이터
- 데이터 구조
	- REST API와 JSON
	- MySQL 데이터베이스의 잘 구성된 데이터
	- MySQL 데이터베이스 테이블의 열 내의 JSON
	- 반정형화된 로그 데이터
	- CSV, 고정 폭 형식(FWF) 및 기타 플랫 파일 형식
	- 플랫 파일의 JSON
	- Kafka의 스트림 출력

분석 프로젝트에 더 적합한 형태로 정형화하기 위해서는 데이터 수집 이 외에도 클렌징, 변환 작업 등의 추가 단계가 파이프라인에서 필요할 수 있다.

JSON과 같은 반정형 데이터가 점점 보편화 되고 있으며 속성-값 구조와 객체의 중첩(nesting)구조의 이점을 가지고 있다. 하지만 RDB와 달리 같은 데이터세트 안의 데이터 구조가 모두 동이랗다는 보장은 없다.

##### 데이터 사이즈

실제로는 대부분의 조직에서 큰 데이터보다 작은 데이터세트를 더 중요하게 생각한다. 또한 크고 작은 데이터세트를 함께 수집하고 모델링하는 것이 일반적이다.

파이프라인에 관련해서 데이터를 대용량과 소용량, 두 가지로만 나눠서 생각하기보다는 스펙트럼 측면에서 생각하는 것이 좋다. (Fuzzy Theory)

##### 데이터 클렌징 작업과 유효성 검사

> garbage in, garbage out

소스 데이터의 한계와 결함을 이해하고 파이프라인의 적절한 부분에서 해결해주는 것이 중요하다.

- 지저분한 데이터의 공통적인 특성
	- 중복되거나 모호한 레코드
	- 고립된 레코드
	- 불완전하거나 누락된 레코드
	- 텍스트 인코딩 오류
	- 일치하지 않는 형식 (-가 있는 전화번호와 없는 전화번호)
	- 레이블이 잘못되었거나 레이블이 지정되지 않은 데이터

1. 최악읠 가정하고 최상을 기대하라
	- 입력 데이터세트에는 수 많은 유효성 및 일관성 문제가 있지만 깨끗한 출력을 위해 데이터를 식별하고 정리하는 파이프라인을 구축한다고 가정한다.
2. 가장 적합한 시스템에서 데이터를 정리하고 검증하라
	- 최신 파이프라인은 데이터 웨어하우징에 대해 ETL 접근방식보다는 ELT 접근방식을 따르는 경향이 있다.
3. 자주 검증하라
	- 데이터 검증을 파이프라인이 끝날 때까지 기다리지 않아야 한다. 반대로 파이프라인 초기에 데이터 검증을 했다고 해도 이후 단계에서 모든 것이 잘 진행될 것이라고 가정하지 말아야 한다.

##### 소스 시스템의 지연 시간 및 대역폭

대부분의 데이터 파이프라인에서는 데이터 수집이 첫 번쨰 단계다. 따라서 소스 시스템과 해당 데이터의 특성을 이해하는 것이 파이프라인을 설계하고 인프라와 관련된 결정을 내리는 첫 번째 단계다.

### 클라우드 데이터 웨어하우스 및 데이터 레이크

- 클라우드에서 데이터 파이프라인, 데이터 레이크, 웨어하우스 및 분석 처리 구축 및 배포가 쉬워짐.
- 지속적인 클라우드 내 스토리지 비용 감소
- redshift, snowflake 및 big query와 같은 확장성이 뛰어난 열 기반 데이터베이스의 등장

데이터 웨어하우스는 사용자가 원하는 질문에 대답할 수 있는 데이터 분석 활동을 지원하기 위해 서로 다른 시스템의 데이터가 모델링되어 저장되는 데이터베이스다. 데이터 웨어하우스의 데이터는 리포팅 및 분석 쿼리를 위해 정형화되고 최적화된다. (fact table, dimension table ?)

데이터 레이크는 데이터가 저장되지만 데이터 웨어하우스처럼 데이터 구조나 쿼리 최적화가 필요 없는 곳이다. 여기에는 다양한 데이터 유형뿐만 아니라 대량의 데이터가 포함될 가능성이 높다. 표준 데이터베이스처럼 정형화된 데이터를 쿼리하는 데 최적화되지는 않았지만 리포팅 및 분석을 위해 이러한 데이터를 저장할 수도 있다.

동일한 데이터 생태계에서 데이터 웨어하우스와 데이터 레이크 모두를 수용할 수 있는 공간이 있으며, 데이터 파이프라인이 둘 사이에서 데이터를 이동하는 경우가 많다.

### 데이터 수집 도구

이 책에서는 다음과 같은 가장 일반적인 도구 및 프레임워크에 대해 설명한다.

- Singer
- Stitch
- Fivetran

이러한 툴일 널리 보급되어 있음에도 불구하고 일부 팀은 데이터를 수집하기 위해 사용자 지정 코드를 구축하기로 결정하기도 하고 일부는 자체 프레임워크를 직접 개발하기도 한다.

### 데이터 변환 및 모델링 도구

데이터 파이프라인은 머신러닝, 분석 및 리포팅과 같은 새로운 목적을 위해 데이터를 변환하고 모델링하는 작업으로 구성된다.

- 데이터 변환
	- ETL또는 ELT 프로세스에서 T에 해당하는 광범위한 용어다. 
- 데이터 모델링
	- 보다 구체적인 데이터 변환 유형이다. 데이터 모델은 데이터 분석을 위해 데이터를 이해하고 최적화된 형식으로 정형화하고 정의한다.

개인 식별 가능 정보(PII)를 보호하기 위해 개인의 전자메일 주소를 최종 목적지에 저장된 해시 값으로 변환하는 것이 좋을 수 있다. 이러한 변환은 일반적으로 수집 프로세스 중에 수행된다.

좀 더 복잡한 데이터 변환 및 데이터 모델링을 위해서는 dbt와 같이 작업을 위해 특별히 설계된 도구와 프레임워클르 찾는 것이 바람직하다.

### 워크플로 오케스트레이션 플랫폼

조직의 데이터 파이프라이의 복잡성과 수가 증가함에 따라 데이터 인프라에 워크플로 오케스트레이션 플랫폼을 도입하는 것이 중요하다. 이러한 플랫폼은 파이프라인에서 작업의 스케줄링 및 흐름을 관리해준다.

> 워크플로 오케스트레이션 플랫폼은 워크플로 관리 시스템(WMS), 오케스트레이션 플랫폼 또는 오케스트레이션 프레임워크라고도 한다.

아파치 에어플로우, Luigi, AWS Glue와 같은 플랫폼은 좀 더 일반적인 용도로 설계되어 다양한 데이터 파이프라인에 사용된다. Kubeflow Pipeline과 같은 플랫폼들은 이보다 구체적인 사용 사례와 플랫폼을 위해 설계되었다.

##### 방향성 비순환 그래프

오케스트레이션 프레임워크는 파이프라인에서 작업의 흐름과 종속성을 그래프로 나타낸다. 그러나 파이프라인 그래프에는 몇 가지 특정 제약 조건이 있다.

파이프라인 단계는 항상 방향성을 가진다. 하나의 작업 또는 여러 개의 작업으로 시작하고 특정 작업으로 끝난다. 이것은 실행 경로와 순서를 보장하기 위해 필요하다. 즉, 모든 종속 작업이 완료되어야만 그 다음 작업이 실행된다.

또한 파이프라인 그래프는 비순환 그래프여야 한다. 작업은 돌아갈 수 없기 때문에 순환할 수 없고 다음으로만 갈 수 있다.

이러한 두 가지 제약 조건을 염두에 두고 오케스트레이션 파이프라인은 방향성 비순환 그래프(DAGs)라는 그래프를 생성한다.

[그림 2-3]

[그림 2-4]

### 데이터 인프라 커스터마이징

조직의 문화와 리소스에 따라 대부분 데이터 인프라를 자체적으로 구축하거나 SaaS 공급 업체를 통해 구매하게 된다. 직접 구축하든 구매하든, 고품질의 데이터 파이프라인을 구축하는 데 필요한 고품질 데이터 인프라를 구축할 수 있다.

중요한 것은 제약조건과 그에 따른 트레이드오프를 이해하는 것이다.
    
# 03장: 일반적인 데이터 파이프라인 패턴

다양한 데이터 소스와 인프라는 도전 과제이자 기회가 될 수 있다. 또한 파이프라인은 각자 다른 목표와 제약 조건을 갖게 되는데, 예를 들어 데이터를 준 실시간으로 처리해야 하는지, 매일 데이터가 업데이트될 수 있는지, 혹은 분석된 데이터를 최종적으로 시각화 대시보드나 머신러닝 모델에 대한 입력값으로 사용할지와 같은 사례를 생각해볼 수 있다.

데이터 파이프라인에는 다양한 사용 사례로 확장 가능한 성공적인 몇 가지 공통 패턴이 있다. 이 장에서는 이러한 패턴을 정의한다.

### ETL과 ELT

ETL과 ELT 모두 데이터 웨어하우징 및 비즈니스 인텔리전스에서 널리 사용되는 패턴이다. 많은 사람들이 이 용어를 많은 파이프라인이 따르는 패턴보다는 데이터 파이프라인 자체와 동의어로 생각하고 사용하기도 한다.

추출(extract) 단계는 로드 및 변환을 준비하기 위해 다양한 소스에서 데이터를 수집한다.

로드(load) 단계는 원본 데이터(ELT의 경우) 또는 완전히 변환된 데이터(ETL의 경우)를 최종 대상으로 가져온다. 어느 쪽이든 최종 결과는 데이터 웨어하우스, 데이터 레이크 또는 기타 대상에 데이터를 로드하는 것이다.

변환(transform) 단계는 분석가, 시각화 도구 또는 파이프라인이 제공하는 모든 사용 사례에 유용하게 쓸 수 있게 각 소스 시스템의 원본 데이터를 결합하고 형식을 지정하는 단계다.

### ETL을 넘어선 ELT의 등장

ETL은 수십 년 동안 데이터 파이프라인 패턴의 표준이었다. 그러면 왜 ELT라는 추가적인 선택지가 등장했을까? 주로 클라우드를 기반으로 하는 최신 유형의 데이터 웨어하우스 이전에는 데이터 팀이 방대한 양의 원본 데이터를 로드하고 이를 사용 가능한 데이터로 변환하는 데 필요한 스토리지나 컴퓨팅 자원이 모두 모여있는 데이터 웨어하우스에 액세스할 수 없었다.

당시 데이터 웨어하우스는 트랜잭션 사용 사례에서 잘 작동하는 행 기반 데이터베이스였으나 분석에서 흔히 볼 수 있는 대용량 쿼리에는 적합하지 않았다. 따라서 데이터는 먼저 소스 시스템에서 추출된 다음 웨어하우스에 로드되어 분석가와 시각화 도구에 의한 최종 데이터 모델링 및 쿼리를 하기 전에 별도의 시스템에서 변환되었다.

오늘날 대부분의 데이터 웨어하우스는 비용 효율적인 방식으로 대규모 데이터세트에 대한 대량 변환을 저장하고 실행할 수 있는 확장성이 뛰어난 열 기반 데이터베이스를 기반으로 한다. 열 기반 데이터베이스의 I/O 효율성, 데이터 압축, 데이터 처리를 위한 여러 병렬 노드에 데이터 및 쿼리를 분산하는 기능 덕분에 상황이 바뀌었다. 따라서 이제 데이터를 추출하고 파이프라인을 완료하는 데 필요한 변환을 수행할 수 있는 데이터 웨어하우스에 로드하는 것에 집중할 수 있게 되었다.

행 기반 데이터 웨어하우스와 열 기반 데이터 웨어하우스 간의 차이가 주는 영향은 정말 크다. ... 데이터베이스의 각 행은 각 레코드의 크기에 따라 하나 이상의 블록으로 디스크에 함께 저장된다. 레코드가 단일 블록보다 작거나 블록 크기로 깔끔하게 나눌 수 없는 경우 일부 디스크 공간을 사용하지 않은 상태로 남긴다.

[그림 3-1]

단일 레코드를 자주 읽고 쓰는 속도가 가장 중요하기 때문에 이 경우 레코드가 블록에 빈 공간을 남기는 비효율성은 합리적인 절충안이다. 그러나 분석에서는 상황이 반대가 된다. 많은 양의 데이터를 드물게 읽고 쓰는 경우가 많기 떄문이다. 또한 분석 쿼리가 테이블에 있는 특정 열의 대부분 또는 전부보다는 단일 열을 필요로 할 가능성이 더 크다.

결국 분석은 데이터를 생성하거나 변경하는 것(OLTP에서와 같이)이 아니라 지표의 파생과 데이터 이해에 관한 것이다.

[그림 3-2]처럼 Snowflake 또는 Amazon Redshift와 같은 열 기반 데이터베이스는 행이 아닌 열 단위로 디스크 블록에 데이터를 저장한다. ... 분석가의 쿼리에 필요한 필터링 및 합산을 수행하기 위해 메모리에 로드할 데이터와 디스크 I/O가 줄어든다. 최종 이점은 저장소를 최적화할 수 있다는 것인데, 각 블록에 행 기반 레코드에서처럼 여러 데이터 유형이 아니라 동일한 데이터 유형이 저장되므로 블록을 남김없이 활용하고 최적으로 압축할 수 있기 때문이다.

열 기반 데이터베이스의 출현은 데이터 웨어하우스 내에서 대규모 데이터세트를 저장, 변환 및 쿼리하는 것이 효율적이라는 의미다. 데이터 엔지니어는 데이터를 추출하고 웨어하우스로 로드하는 전용 파이프라인 단계를 구축하여 활용할 수 있다. 그리고 데이터베이스라는 범위 안에서 분석가와 데이터 과학자들은 좀 더 편하게 데이터를 변환, 모델링 및 쿼리할 수 있다. 따라서 ETL는 머신러닝 및 데이터 제품 개발과 같은 다른 사용 사례뿐만 아니라 데이터 웨어하우스 파이프라인을 위한 이상적인 패턴으로 자리 잡았다.

### EtLT 하위 패턴



### 데이터 분석을 위한 ELT
### 데이터 과학을 위한 ELT
### 데이터 제품 및 머신러닝을 위한 ELT

##### 머신러닝 파이프라인의 단계
##### 파이프라인에 피드백 통합
##### ML 파이프라인에 대한 추가 자료
    
# 04장: 데이터 수집: 데이터 추출
-   파이썬 환경 설정
-   클라우드 파일 스토리지 설정
-   MySQL 데이터베이스에서 데이터 추출
	-   전체 또는 증분 MySQL 테이블 추출
	-   MySQL 데이터의 이진 로그 복제
-   PostgreSQL 데이터베이스에서 데이터 추출
	-   전체 또는 증분 Postgres 테이블 추출
	-   Write-Ahead 로그를 사용한 데이터 복제
-   MongoDB에서 데이터 추출
-   REST API에서 데이터 추출
-   카프카 및 Debezium을 통한 스트리밍 데이터 수집
    
# 05장: 데이터 수집: 데이터 로드
-   Amazon Redshift 웨어하우스를 대상으로 구성
-   Redshift 웨어하우스에 데이터 로드
	-   증분 및 전체 로드
	-   CDC 로그에서 추출한 데이터 로드
-   Snowflake 웨어하우스를 대상으로 구성3
-   Snowflake 데이터 웨어하우스에 데이터 로드
-   파일 스토리지를 데이터 레이크로 사용
-   오픈 소스 프레임워크
-   상업적 대안
    
# 06장: 데이터 변환하기
-   비문맥적 변환
	-   테이블에서 레코드 중복 제거
	-   URL 파싱
-   언제 변환할 것인가, 수집 중 혹은 수집 후?
-   데이터 모델링 기초
	-   주요 데이터 모델링 용어
	-   완전히 새로 고침 된 데이터 모델링
	-   완전히 새로 고침 된 데이터의 차원을 천천히 변경
	-   증분 수집된 데이터 모델링
	-   추가 전용(Append-only) 데이터 모델링
	-   변경 캡처 데이터 모델링
    
# 07장: 파이프라인 오케스트레이션
-   방향성 비순환 그래프
-   아파치 에어플로우 설정 및 개요
	-   설치 및 구성
	-   에어플로우 데이터베이스
	-   웹 서버 및 UI
	-   스케줄러
	-   실행기(Executors)
	-   연산자(Operators)
-   에어플로우 DAG 구축
	-   간단한 DAG
	-   ELT 파이프라인 DAG
-   추가 파이프라인 작업
	-   경고 및 알림
	-   데이터 유효성 검사
-   고급 오케스트레이션 구성
	-   결합된 파이프라인 작업 대 결합되지 않은 파이프라인 작업
	-   DAG를 분할해야 하는 경우
	-   센서로 여러 DAG 조정
-   관리형 에어플로우 옵션
-   기타 오케스트레이션 프레임워크
-   일찍 그리고 자주 검증할 것
    
# 08장: 파이프라인의 데이터 검증
-   소스 시스템 데이터 품질
-   데이터 수집 위험
-   데이터 분석가 검증 활성화
-   간단한 검증 프레임워크
	-   유효성 검사기 프레임워크 코드
	-   검증 테스트의 구조
	-   검증 테스트 실행
	-   에어플로우 DAG에서의 사용
	-   파이프라인을 중단해야 할 때와 경고하고 계속해야 할 때
	-   프레임워크의 확장
-   검증 테스트 예제
	-   수집 후 중복된 레코드
	-   수집 후의 예기치 않은 행 개수
	-   지표 값 변동
-   상용 및 오픈 소스 데이터 검증 프레임워크
-   소스 시스템의 변경 사항 처리
    
# 09장: 파이프라인 유지 관리 모범 사례
-   추상화 도입
-   데이터 계약 유지 관리
-   Schema-on-Read의 고려사항
-   확장 복잡성
	-   데이터 수집 표준화
	-   데이터 모델링 로직의 재사용
	-   종속성 무결성 보장
-   중요 파이프라인 지표
    
# 10장: 파이프라인 성능 측정 및 모니터링
-   데이터 웨어하우스 준비
	-   데이터 인프라 스키마
-   성능 데이터 로깅 및 수집
	-   에어플로우에서 DAG 실행 기록 수집
	-   데이터 유효성 검사기에 로깅 추가
-   성능 데이터 변환
	-   DAG 성공률
	-   시간 경과에 따른 DAG 런타임 변경
	-   검증 테스트 볼륨 및 성공률
-   성능 파이프라인 조정
	-   DAG의 성능
-   성능 투명성